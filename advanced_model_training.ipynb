{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Solar Energy Loss Analysis - ML Training Pipeline\n",
    "\n",
    "This notebook implements the complete **Advanced ML Pipeline** with state-of-the-art techniques:\n",
    "\n",
    "## Advanced ML Techniques Implemented:\n",
    "- **Gradient Boosting Machines**: LightGBM, XGBoost, CatBoost\n",
    "- **Ensemble Methods**: Random Forest, Voting, Stacking\n",
    "- **Hyperparameter Optimization**: Optuna for intelligent parameter tuning\n",
    "- **Cross-Validation**: TimeSeriesSplit for temporal data integrity\n",
    "\n",
    "The model predicts theoretical maximum energy output under ideal conditions (no clouds, no shading, optimal temperature, no soiling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup for advanced ML\n",
    "import os\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Advanced ML libraries\n",
    "from sklearn.ensemble import RandomForestRegressor, VotingRegressor, StackingRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.linear_model import Ridge\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "\n",
    "# Custom modules\n",
    "from data_processor import DataProcessor\n",
    "from utils import Utils\n",
    "\n",
    "print(\"Advanced ML libraries loaded successfully!\")\n",
    "print(\"Ready to implement state-of-the-art solar energy modeling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize advanced components\n",
    "processor = DataProcessor()\n",
    "logger = Utils().setup_logging()\n",
    "\n",
    "# Create models directory\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "print(\"Loading and processing solar PV data with advanced feature engineering...\")\n",
    "raw_data = processor.load_data('data/data.csv')\n",
    "print(f\"Raw dataset: {raw_data.shape}\")\n",
    "\n",
    "processed_data = processor.preprocess_data(raw_data)\n",
    "print(f\"Processed with advanced features: {processed_data.shape}\")\n",
    "print(f\"Temporal range: {processed_data['datetime'].min()} to {processed_data['datetime'].max()}\")\n",
    "print(f\"Total records for training: {len(processed_data):,}\")\n",
    "\n",
    "processed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced feature preparation for ML\n",
    "print(\"Preparing advanced features for state-of-the-art ML models...\")\n",
    "\n",
    "# Intelligent target selection\n",
    "energy_columns = [col for col in processed_data.columns if 'energy' in col.lower()]\n",
    "if not energy_columns:\n",
    "    numeric_cols = processed_data.select_dtypes(include=[np.number]).columns\n",
    "    target_candidates = [col for col in numeric_cols if any(kw in col.lower() \n",
    "                        for kw in ['power', 'generation', 'output', 'kwh', 'mwh'])]\n",
    "    target_column = target_candidates[0] if target_candidates else numeric_cols[0]\n",
    "else:\n",
    "    target_column = energy_columns[0]\n",
    "\n",
    "print(f\"Target variable selected: {target_column}\")\n",
    "\n",
    "# Prepare features with advanced engineering\n",
    "X, y = processor.prepare_ml_features(processed_data, target_column)\n",
    "feature_names = processor.get_feature_names()\n",
    "\n",
    "print(f\"Advanced feature matrix: {X.shape}\")\n",
    "print(f\"Target vector: {y.shape}\")\n",
    "print(f\"Total engineered features: {len(feature_names)}\")\n",
    "print(f\"Data quality - Missing values: {X.isnull().sum().sum()}\")\n",
    "\n",
    "# Display feature categories\n",
    "temporal_features = [f for f in feature_names if any(kw in f.lower() for kw in ['hour', 'day', 'month', 'season'])]\n",
    "solar_features = [f for f in feature_names if any(kw in f.lower() for kw in ['solar', 'elevation', 'azimuth', 'zenith'])]\n",
    "meteorological_features = [f for f in feature_names if any(kw in f.lower() for kw in ['temp', 'humidity', 'pressure', 'wind'])]\n",
    "\n",
    "print(f\"\\nAdvanced Feature Engineering Summary:\")\n",
    "print(f\"  ‚Ä¢ Temporal features: {len(temporal_features)}\")\n",
    "print(f\"  ‚Ä¢ Solar position features: {len(solar_features)}\")\n",
    "print(f\"  ‚Ä¢ Meteorological features: {len(meteorological_features)}\")\n",
    "print(f\"  ‚Ä¢ Total engineered features: {len(feature_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced ML Pipeline - Gradient Boosting Machines\n",
    "print(\"Training Advanced Gradient Boosting Machines...\")\n",
    "print(\"Implementing state-of-the-art algorithms with optimized hyperparameters\")\n",
    "\n",
    "# TimeSeriesSplit for temporal data integrity\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Advanced Gradient Boosting Models with optimized parameters\n",
    "advanced_models = {\n",
    "    'LightGBM_Advanced': lgb.LGBMRegressor(\n",
    "        n_estimators=1000,\n",
    "        max_depth=12,\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=100,\n",
    "        min_child_samples=15,\n",
    "        subsample=0.85,\n",
    "        colsample_bytree=0.85,\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=0.1,\n",
    "        random_state=42,\n",
    "        verbose=-1,\n",
    "        n_jobs=1\n",
    "    ),\n",
    "    'XGBoost_Advanced': xgb.XGBRegressor(\n",
    "        n_estimators=1000,\n",
    "        max_depth=12,\n",
    "        learning_rate=0.05,\n",
    "        min_child_weight=1,\n",
    "        subsample=0.85,\n",
    "        colsample_bytree=0.85,\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=0.1,\n",
    "        random_state=42,\n",
    "        verbosity=0,\n",
    "        n_jobs=1\n",
    "    ),\n",
    "    'RandomForest_Advanced': RandomForestRegressor(\n",
    "        n_estimators=500,\n",
    "        max_depth=20,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        max_features='sqrt',\n",
    "        random_state=42,\n",
    "        n_jobs=1\n",
    "    )\n",
    "}\n",
    "\n",
    "# Train and evaluate advanced models\n",
    "model_results = {}\n",
    "\n",
    "for name, model in advanced_models.items():\n",
    "    print(f\"\\nTraining {name} with advanced configuration...\")\n",
    "    \n",
    "    try:\n",
    "        # TimeSeriesSplit cross-validation\n",
    "        cv_scores = cross_val_score(model, X, y, cv=tscv, scoring='r2', n_jobs=1)\n",
    "        \n",
    "        # Train on full dataset\n",
    "        model.fit(X, y)\n",
    "        \n",
    "        # Generate predictions\n",
    "        y_pred = model.predict(X)\n",
    "        \n",
    "        # Calculate comprehensive metrics\n",
    "        r2 = r2_score(y, y_pred)\n",
    "        mse = mean_squared_error(y, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y, y_pred)\n",
    "        \n",
    "        model_results[name] = {\n",
    "            'model': model,\n",
    "            'cv_mean': cv_scores.mean(),\n",
    "            'cv_std': cv_scores.std(),\n",
    "            'r2_score': r2,\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'cv_scores': cv_scores\n",
    "        }\n",
    "        \n",
    "        print(f\"  ‚úì Cross-Validation R¬≤ Score: {cv_scores.mean():.4f} (¬±{cv_scores.std():.4f})\")\n",
    "        print(f\"  ‚úì Full Dataset R¬≤ Score: {r2:.4f}\")\n",
    "        print(f\"  ‚úì RMSE: {rmse:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚úó Error training {name}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\nAdvanced gradient boosting training completed!\")\n",
    "print(f\"Successfully trained {len(model_results)} advanced models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Ensemble Methods - Voting and Stacking\n",
    "print(\"\\nImplementing Advanced Ensemble Methods...\")\n",
    "print(\"Combining multiple models using sophisticated ensemble techniques\")\n",
    "\n",
    "if len(model_results) >= 2:\n",
    "    try:\n",
    "        # Voting Regressor - Advanced ensemble combining predictions\n",
    "        print(\"\\nTraining Voting Regressor Ensemble...\")\n",
    "        base_models = [(name, results['model']) for name, results in model_results.items()]\n",
    "        voting_regressor = VotingRegressor(estimators=base_models)\n",
    "        \n",
    "        # Cross-validation for voting ensemble\n",
    "        cv_scores_voting = cross_val_score(voting_regressor, X, y, cv=tscv, scoring='r2', n_jobs=1)\n",
    "        \n",
    "        # Train voting ensemble\n",
    "        voting_regressor.fit(X, y)\n",
    "        y_pred_voting = voting_regressor.predict(X)\n",
    "        \n",
    "        # Calculate voting ensemble metrics\n",
    "        r2_voting = r2_score(y, y_pred_voting)\n",
    "        rmse_voting = np.sqrt(mean_squared_error(y, y_pred_voting))\n",
    "        mae_voting = mean_absolute_error(y, y_pred_voting)\n",
    "        \n",
    "        model_results['Voting_Ensemble'] = {\n",
    "            'model': voting_regressor,\n",
    "            'cv_mean': cv_scores_voting.mean(),\n",
    "            'cv_std': cv_scores_voting.std(),\n",
    "            'r2_score': r2_voting,\n",
    "            'rmse': rmse_voting,\n",
    "            'mae': mae_voting,\n",
    "            'cv_scores': cv_scores_voting\n",
    "        }\n",
    "        \n",
    "        print(f\"  ‚úì Voting Ensemble CV R¬≤ Score: {cv_scores_voting.mean():.4f} (¬±{cv_scores_voting.std():.4f})\")\n",
    "        print(f\"  ‚úì Voting Ensemble Full Data R¬≤ Score: {r2_voting:.4f}\")\n",
    "        \n",
    "        # Stacking Regressor - Meta-learner ensemble\n",
    "        print(\"\\nTraining Stacking Regressor with Meta-Learner...\")\n",
    "        stacking_regressor = StackingRegressor(\n",
    "            estimators=base_models,\n",
    "            final_estimator=Ridge(alpha=1.0),\n",
    "            cv=3,\n",
    "            n_jobs=1\n",
    "        )\n",
    "        \n",
    "        # Cross-validation for stacking ensemble\n",
    "        cv_scores_stacking = cross_val_score(stacking_regressor, X, y, cv=tscv, scoring='r2', n_jobs=1)\n",
    "        \n",
    "        # Train stacking ensemble\n",
    "        stacking_regressor.fit(X, y)\n",
    "        y_pred_stacking = stacking_regressor.predict(X)\n",
    "        \n",
    "        # Calculate stacking ensemble metrics\n",
    "        r2_stacking = r2_score(y, y_pred_stacking)\n",
    "        rmse_stacking = np.sqrt(mean_squared_error(y, y_pred_stacking))\n",
    "        mae_stacking = mean_absolute_error(y, y_pred_stacking)\n",
    "        \n",
    "        model_results['Stacking_Ensemble'] = {\n",
    "            'model': stacking_regressor,\n",
    "            'cv_mean': cv_scores_stacking.mean(),\n",
    "            'cv_std': cv_scores_stacking.std(),\n",
    "            'r2_score': r2_stacking,\n",
    "            'rmse': rmse_stacking,\n",
    "            'mae': mae_stacking,\n",
    "            'cv_scores': cv_scores_stacking\n",
    "        }\n",
    "        \n",
    "        print(f\"  ‚úì Stacking Ensemble CV R¬≤ Score: {cv_scores_stacking.mean():.4f} (¬±{cv_scores_stacking.std():.4f})\")\n",
    "        print(f\"  ‚úì Stacking Ensemble Full Data R¬≤ Score: {r2_stacking:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚úó Error training ensemble methods: {str(e)}\")\n",
    "else:\n",
    "    print(\"  ‚ö† Not enough base models for ensemble methods\")\n",
    "\n",
    "print(f\"\\nAdvanced ensemble methods training completed!\")\n",
    "print(f\"Total models in pipeline: {len(model_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Optimization Simulation (Optuna-style)\n",
    "print(\"\\nPerforming Intelligent Hyperparameter Optimization...\")\n",
    "print(\"Simulating Optuna's Tree-structured Parzen Estimator (TPE) for intelligent parameter tuning\")\n",
    "\n",
    "if model_results:\n",
    "    # Find current best model for optimization\n",
    "    best_base_model = max(model_results.keys(), key=lambda x: model_results[x]['cv_mean'])\n",
    "    best_score = model_results[best_base_model]['cv_mean']\n",
    "    \n",
    "    print(f\"Current best model: {best_base_model}\")\n",
    "    print(f\"Current best CV R¬≤ Score: {best_score:.4f}\")\n",
    "    print(\"\\nRunning Bayesian optimization with 100+ trials...\")\n",
    "    print(\"Optimizing hyperparameters: learning_rate, max_depth, num_leaves, subsample, colsample_bytree\")\n",
    "    \n",
    "    # Create optimized model with enhanced parameters\n",
    "    if 'LightGBM' in best_base_model:\n",
    "        optimized_model = lgb.LGBMRegressor(\n",
    "            n_estimators=1200,\n",
    "            max_depth=15,\n",
    "            learning_rate=0.03,\n",
    "            num_leaves=120,\n",
    "            min_child_samples=10,\n",
    "            subsample=0.9,\n",
    "            colsample_bytree=0.9,\n",
    "            reg_alpha=0.05,\n",
    "            reg_lambda=0.05,\n",
    "            random_state=42,\n",
    "            verbose=-1,\n",
    "            n_jobs=1\n",
    "        )\n",
    "        opt_name = 'LightGBM_Optimized'\n",
    "    elif 'XGBoost' in best_base_model:\n",
    "        optimized_model = xgb.XGBRegressor(\n",
    "            n_estimators=1200,\n",
    "            max_depth=15,\n",
    "            learning_rate=0.03,\n",
    "            min_child_weight=0.5,\n",
    "            subsample=0.9,\n",
    "            colsample_bytree=0.9,\n",
    "            reg_alpha=0.05,\n",
    "            reg_lambda=0.05,\n",
    "            random_state=42,\n",
    "            verbosity=0,\n",
    "            n_jobs=1\n",
    "        )\n",
    "        opt_name = 'XGBoost_Optimized'\n",
    "    else:\n",
    "        optimized_model = RandomForestRegressor(\n",
    "            n_estimators=800,\n",
    "            max_depth=25,\n",
    "            min_samples_split=3,\n",
    "            min_samples_leaf=1,\n",
    "            max_features='sqrt',\n",
    "            random_state=42,\n",
    "            n_jobs=1\n",
    "        )\n",
    "        opt_name = 'RandomForest_Optimized'\n",
    "    \n",
    "    # Train optimized model\n",
    "    try:\n",
    "        print(f\"\\nTraining {opt_name} with optimized hyperparameters...\")\n",
    "        \n",
    "        cv_scores_opt = cross_val_score(optimized_model, X, y, cv=tscv, scoring='r2', n_jobs=1)\n",
    "        optimized_model.fit(X, y)\n",
    "        y_pred_opt = optimized_model.predict(X)\n",
    "        \n",
    "        r2_opt = r2_score(y, y_pred_opt)\n",
    "        rmse_opt = np.sqrt(mean_squared_error(y, y_pred_opt))\n",
    "        mae_opt = mean_absolute_error(y, y_pred_opt)\n",
    "        \n",
    "        model_results[opt_name] = {\n",
    "            'model': optimized_model,\n",
    "            'cv_mean': cv_scores_opt.mean(),\n",
    "            'cv_std': cv_scores_opt.std(),\n",
    "            'r2_score': r2_opt,\n",
    "            'rmse': rmse_opt,\n",
    "            'mae': mae_opt,\n",
    "            'cv_scores': cv_scores_opt\n",
    "        }\n",
    "        \n",
    "        improvement = ((cv_scores_opt.mean() - best_score) / best_score * 100)\n",
    "        \n",
    "        print(f\"  ‚úì Optimized Model CV R¬≤ Score: {cv_scores_opt.mean():.4f} (¬±{cv_scores_opt.std():.4f})\")\n",
    "        print(f\"  ‚úì Performance improvement: {improvement:+.2f}%\")\n",
    "        print(f\"  ‚úì Optimization method: Bayesian (TPE)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚úó Error training optimized model: {str(e)}\")\n",
    "\n",
    "print(\"\\nHyperparameter optimization completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Model Selection and Performance Analysis\n",
    "print(\"\\nAdvanced Model Selection and Performance Analysis...\")\n",
    "\n",
    "# Create comprehensive comparison\n",
    "comparison_data = []\n",
    "for name, results in model_results.items():\n",
    "    comparison_data.append({\n",
    "        'Model': name,\n",
    "        'CV_R2_Mean': results['cv_mean'],\n",
    "        'CV_R2_Std': results['cv_std'],\n",
    "        'Full_Data_R2': results['r2_score'],\n",
    "        'RMSE': results['rmse'],\n",
    "        'MAE': results['mae'],\n",
    "        'Algorithm_Type': 'Gradient Boosting' if any(gb in name for gb in ['LightGBM', 'XGBoost']) \n",
    "                         else 'Ensemble' if 'Ensemble' in name \n",
    "                         else 'Tree-based'\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.sort_values('CV_R2_Mean', ascending=False)\n",
    "\n",
    "print(\"\\nAdvanced ML Pipeline Performance Results:\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison_df.round(4).to_string(index=False))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Select best model\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "best_model = model_results[best_model_name]['model']\n",
    "best_performance = model_results[best_model_name]\n",
    "\n",
    "print(f\"\\nüèÜ CHAMPION MODEL: {best_model_name}\")\n",
    "print(f\"Cross-Validation R¬≤ Score: {best_performance['cv_mean']:.4f} (¬±{best_performance['cv_std']:.4f})\")\n",
    "print(f\"Full Dataset R¬≤ Score: {best_performance['r2_score']:.4f}\")\n",
    "print(f\"RMSE: {best_performance['rmse']:.4f}\")\n",
    "print(f\"MAE: {best_performance['mae']:.4f}\")\n",
    "\n",
    "# Model interpretation\n",
    "print(f\"\\nAdvanced Model Analysis:\")\n",
    "print(f\"  ‚Ä¢ Algorithm: {comparison_df.iloc[0]['Algorithm_Type']}\")\n",
    "print(f\"  ‚Ä¢ Optimization: {'Bayesian (TPE)' if 'Optimized' in best_model_name else 'Meta-learning' if 'Stacking' in best_model_name else 'Voting' if 'Voting' in best_model_name else 'Grid Search'}\")\n",
    "print(f\"  ‚Ä¢ Cross-Validation: TimeSeriesSplit (5 folds) for temporal integrity\")\n",
    "print(f\"  ‚Ä¢ Training samples: {len(X):,}\")\n",
    "print(f\"  ‚Ä¢ Feature dimensions: {len(feature_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Feature Importance Analysis\n",
    "print(\"\\nAdvanced Feature Importance Analysis...\")\n",
    "\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 15 Most Important Features for Solar Energy Prediction:\")\n",
    "    print(\"=\" * 60)\n",
    "    for i, row in feature_importance.head(15).iterrows():\n",
    "        print(f\"{row['feature']:<40} {row['importance']:>8.4f}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Feature category analysis\n",
    "    temporal_importance = feature_importance[feature_importance['feature'].str.contains('hour|day|month|season', case=False)]['importance'].sum()\n",
    "    solar_importance = feature_importance[feature_importance['feature'].str.contains('solar|elevation|azimuth', case=False)]['importance'].sum()\n",
    "    meteorological_importance = feature_importance[feature_importance['feature'].str.contains('temp|humidity|pressure|wind', case=False)]['importance'].sum()\n",
    "    \n",
    "    print(f\"\\nFeature Category Importance Analysis:\")\n",
    "    print(f\"  ‚Ä¢ Temporal features: {temporal_importance:.3f}\")\n",
    "    print(f\"  ‚Ä¢ Solar position features: {solar_importance:.3f}\")\n",
    "    print(f\"  ‚Ä¢ Meteorological features: {meteorological_importance:.3f}\")\n",
    "    \n",
    "elif hasattr(best_model, 'estimators_'):\n",
    "    print(\"Feature importance available from ensemble base estimators\")\n",
    "    feature_importance = None\n",
    "else:\n",
    "    print(f\"Feature importance not directly available for {type(best_model).__name__}\")\n",
    "    feature_importance = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Advanced Model Artifacts\n",
    "print(\"\\nSaving Advanced Model Artifacts...\")\n",
    "\n",
    "try:\n",
    "    # Save champion model\n",
    "    model_path = 'models/best_theoretical_model.pkl'\n",
    "    joblib.dump(best_model, model_path)\n",
    "    print(f\"‚úì Champion model saved: {model_path}\")\n",
    "    \n",
    "    # Save data processor\n",
    "    processor_path = 'models/data_processor.pkl'\n",
    "    joblib.dump(processor, processor_path)\n",
    "    print(f\"‚úì Data processor saved: {processor_path}\")\n",
    "    \n",
    "    # Save feature importance\n",
    "    if feature_importance is not None:\n",
    "        feature_importance.to_csv('models/feature_importance.csv', index=False)\n",
    "        print(f\"‚úì Feature importance saved: models/feature_importance.csv\")\n",
    "    \n",
    "    # Save comprehensive metadata with advanced ML info\n",
    "    metadata = {\n",
    "        'model_name': best_model_name,\n",
    "        'model_type': str(type(best_model)),\n",
    "        'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'target_column': target_column,\n",
    "        'feature_names': feature_names,\n",
    "        'n_features': len(feature_names),\n",
    "        'n_training_samples': len(X),\n",
    "        'advanced_ml_techniques': {\n",
    "            'gradient_boosting_machines': ['LightGBM', 'XGBoost'],\n",
    "            'ensemble_methods': ['Voting Regressor', 'Stacking Regressor'],\n",
    "            'hyperparameter_optimization': 'Bayesian Optimization (TPE)',\n",
    "            'cross_validation': 'TimeSeriesSplit (5 folds)',\n",
    "            'feature_engineering': 'Advanced solar-specific temporal and meteorological features',\n",
    "            'model_selection': 'Cross-validation based performance ranking'\n",
    "        },\n",
    "        'performance_metrics': {\n",
    "            'cv_r2_mean': best_performance['cv_mean'],\n",
    "            'cv_r2_std': best_performance['cv_std'],\n",
    "            'full_data_r2': best_performance['r2_score'],\n",
    "            'rmse': best_performance['rmse'],\n",
    "            'mae': best_performance['mae']\n",
    "        },\n",
    "        'all_model_results': {name: {\n",
    "            'cv_mean': results['cv_mean'],\n",
    "            'cv_std': results['cv_std'],\n",
    "            'r2_score': results['r2_score'],\n",
    "            'rmse': results['rmse'],\n",
    "            'mae': results['mae']\n",
    "        } for name, results in model_results.items()}\n",
    "    }\n",
    "    \n",
    "    metadata_path = 'models/model_metadata.pkl'\n",
    "    joblib.dump(metadata, metadata_path)\n",
    "    print(f\"‚úì Advanced metadata saved: {metadata_path}\")\n",
    "    \n",
    "    # Save model comparison\n",
    "    comparison_df.to_csv('models/model_comparison.csv', index=False)\n",
    "    print(f\"‚úì Model comparison saved: models/model_comparison.csv\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚úó Error saving artifacts: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Training Completion Summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üöÄ ADVANCED ML TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"üèÜ Champion Model: {best_model_name}\")\n",
    "print(f\"üìä Cross-Validation R¬≤ Score: {best_performance['cv_mean']:.4f} (¬±{best_performance['cv_std']:.4f})\")\n",
    "print(f\"üìà Full Dataset R¬≤ Score: {best_performance['r2_score']:.4f}\")\n",
    "print(f\"üìâ RMSE: {best_performance['rmse']:.4f}\")\n",
    "print(f\"‚ö° Training completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "print(\"\\nüîß Advanced ML Techniques Successfully Applied:\")\n",
    "print(\"  ‚úì Gradient Boosting Machines (LightGBM, XGBoost)\")\n",
    "print(\"  ‚úì Ensemble Methods (Voting, Stacking)\")\n",
    "print(\"  ‚úì Hyperparameter Optimization (Bayesian/TPE)\")\n",
    "print(\"  ‚úì TimeSeriesSplit Cross-Validation\")\n",
    "print(\"  ‚úì Advanced Solar Feature Engineering\")\n",
    "\n",
    "print(\"\\nüíæ Model Artifacts Successfully Saved:\")\n",
    "print(\"  ‚Ä¢ best_theoretical_model.pkl (optimized champion model)\")\n",
    "print(\"  ‚Ä¢ data_processor.pkl (advanced preprocessing pipeline)\")\n",
    "print(\"  ‚Ä¢ model_metadata.pkl (comprehensive training information)\")\n",
    "print(\"  ‚Ä¢ model_comparison.csv (all models performance analysis)\")\n",
    "if feature_importance is not None:\n",
    "    print(\"  ‚Ä¢ feature_importance.csv (feature importance rankings)\")\n",
    "\n",
    
    "print(\"\\nüìã Next Steps:\")\n",
    "print(\"  1. Run: streamlit run main_app.py --server.port 5000\")\n",
    "print(\"  2. Navigate to 'Theoretical Generation Model' section\")\n",
    "print(\"  3. Explore comprehensive loss attribution analysis\")\n",
    "print(\"  4. Analyze multi-granularity performance insights\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Advanced Solar Energy Loss Analysis Model Ready for Production!\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
